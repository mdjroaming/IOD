{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nji1a9ULLtCA"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnsX1AWKLtCE"
   },
   "source": [
    "# Lab 8.5: Text Classification\n",
    "\n",
    "In this lab you will implement different types of feature engineering for text classification:\n",
    "* Count vectors\n",
    "* TF-IDF vectors (word level, n-gram level, character level)\n",
    "* Text/NLP based features\n",
    "* Topic models\n",
    "  \n",
    "The following classification algorithms will be applied to the count and TF-IDF vector features:\n",
    "* Na√Øve Bayes\n",
    "* Logistic Regression\n",
    "* Support Vector Machine\n",
    "* Random Forest\n",
    "* Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pm8PttyLtCI"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:38:33.182995Z",
     "start_time": "2019-06-17T01:38:30.045388Z"
    },
    "id": "EUANiH6zLtCK"
   },
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58bUNQA0LtCV"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqU7d_qcLtCX"
   },
   "source": [
    "Sample:\n",
    "\n",
    "    __label__2 Stuning even for the non-gamer: This sound ...\n",
    "    __label__2 The best soundtrack ever to anything.: I'm ...\n",
    "    __label__2 Amazing!: This soundtrack is my favorite m ...\n",
    "    __label__2 Excellent Soundtrack: I truly like this so ...\n",
    "    __label__2 Remember, Pull Your Jaw Off The Floor Afte ...\n",
    "    __label__2 an absolute masterpiece: I am quite sure a ...\n",
    "    __label__1 Buyer beware: This is a self-published boo ...\n",
    "    . . .\n",
    "    \n",
    "There are only two **labels**:\n",
    "- `__label__1`\n",
    "- `__label__2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:38:42.024845Z",
     "start_time": "2019-06-17T01:38:41.896098Z"
    },
    "id": "rwWFJprZLtCZ"
   },
   "outputs": [],
   "source": [
    "## Loading the data\n",
    "\n",
    "df_corpus = pd.read_fwf(\n",
    "    filepath_or_buffer = 'corpus.txt',\n",
    "    colspecs = [(9, 10),   # label: get only the numbers 1 or 2\n",
    "                (11, 9000) # text: makes the it big enough to get to the end of the line\n",
    "               ],\n",
    "    header = 0,\n",
    "    names = ['label', 'text'],\n",
    "    lineterminator = '\\n'\n",
    ")\n",
    "\n",
    "# convert label from [1, 2] to [0, 1]\n",
    "df_corpus['label'] = df_corpus['label'] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mILVIHomLtCf"
   },
   "source": [
    "## Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:39:24.213192Z",
     "start_time": "2019-06-17T01:39:24.209202Z"
    },
    "id": "G9_8RbOeLtCh"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>an absolute masterpiece: I am quite sure any o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  The best soundtrack ever to anything.: I'm rea...\n",
       "1      1  Amazing!: This soundtrack is my favorite music...\n",
       "2      1  Excellent Soundtrack: I truly like this soundt...\n",
       "3      1  Remember, Pull Your Jaw Off The Floor After He...\n",
       "4      1  an absolute masterpiece: I am quite sure any o..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ANSWER\n",
    "\n",
    "df_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>1</td>\n",
       "      <td>A revelation of life in small town America in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1</td>\n",
       "      <td>Great biography of a very interesting journali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0</td>\n",
       "      <td>Interesting Subject; Poor Presentation: You'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0</td>\n",
       "      <td>Don't buy: The box looked used and it is obvio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>1</td>\n",
       "      <td>Beautiful Pen and Fast Delivery.: The pen was ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "9994      1  A revelation of life in small town America in ...\n",
       "9995      1  Great biography of a very interesting journali...\n",
       "9996      0  Interesting Subject; Poor Presentation: You'd ...\n",
       "9997      0  Don't buy: The box looked used and it is obvio...\n",
       "9998      1  Beautiful Pen and Fast Delivery.: The pen was ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9999 entries, 0 to 9998\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   9999 non-null   int64 \n",
      " 1   text    9999 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 156.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_corpus.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YmYgG2pLtCu"
   },
   "source": [
    "## Split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and Labels\n",
    "X = df_corpus['text']\n",
    "y = df_corpus['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:39:40.103737Z",
     "start_time": "2019-06-17T01:39:40.100739Z"
    },
    "id": "j5vErjWFLtCy"
   },
   "outputs": [],
   "source": [
    "## ANSWER\n",
    "## split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nUp6oDOLtC1"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKd9yTnyLtC2"
   },
   "source": [
    "### Count Vectors as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:40:32.674674Z",
     "start_time": "2019-06-17T01:40:31.098889Z"
    },
    "id": "DU2RqqDjLtC3"
   },
   "outputs": [],
   "source": [
    "# create a count vectorizer object\n",
    "count_vect = CountVectorizer(token_pattern = r'\\w{1,}')\n",
    "\n",
    "# Learn a vocabulary dictionary of all tokens in the raw documents\n",
    "count_vect.fit(X_train)\n",
    "\n",
    "# Transform documents to document-term matrix.\n",
    "X_train_count = count_vect.transform(X_train)\n",
    "X_test_count = count_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJs6al0ILtC5"
   },
   "source": [
    "### TF-IDF Vectors as features\n",
    "- Word level\n",
    "- N-Gram level\n",
    "- Character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:40:36.088730Z",
     "start_time": "2019-06-17T01:40:34.519925Z"
    },
    "id": "myjfdfP_LtC6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_features=5000, token_pattern='\\\\w{1,}')\n",
      "CPU times: total: 2.39 s\n",
      "Wall time: 3.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer = 'word',\n",
    "                             token_pattern = r'\\w{1,}',\n",
    "                             max_features = 5000)\n",
    "print(tfidf_vect)\n",
    "\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf  = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:40:57.505221Z",
     "start_time": "2019-06-17T01:40:49.387393Z"
    },
    "id": "-h16dUaVLtC_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_features=5000, ngram_range=(2, 3), token_pattern='\\\\w{1,}')\n",
      "CPU times: total: 9.98 s\n",
      "Wall time: 13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ngram level tf-idf\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer = 'word',\n",
    "                                   token_pattern = r'\\w{1,}',\n",
    "                                   ngram_range = (2, 3),\n",
    "                                   max_features = 5000)\n",
    "print(tfidf_vect_ngram)\n",
    "\n",
    "tfidf_vect_ngram.fit(X_train)\n",
    "X_train_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "X_test_tfidf_ngram  = tfidf_vect_ngram.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:41:10.209071Z",
     "start_time": "2019-06-17T01:40:59.211484Z"
    },
    "id": "Y7rmIt49LtDC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='char', max_features=5000, ngram_range=(2, 3))\n",
      "CPU times: total: 15.2 s\n",
      "Wall time: 21.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer = 'char',\n",
    "                                         ngram_range = (2, 3),\n",
    "                                         max_features = 5000)\n",
    "print(tfidf_vect_ngram_chars)\n",
    "\n",
    "tfidf_vect_ngram_chars.fit(X_train)\n",
    "X_train_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(X_train)\n",
    "X_test_tfidf_ngram_chars  = tfidf_vect_ngram_chars.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Pck1cuvLtDH"
   },
   "source": [
    "### Text / NLP based features\n",
    "\n",
    "Create some other features.\n",
    "\n",
    "char_count = Number of Characters in Text\n",
    "\n",
    "word_count = Number of Words in Text\n",
    "\n",
    "word_density = Average Number of Char in Words\n",
    "\n",
    "punctuation_count = Number of Punctuation in Text\n",
    "\n",
    "title_word_count = Number of Words in Title\n",
    "\n",
    "uppercase_word_count = Number of Upperwords in Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:43:52.813132Z",
     "start_time": "2019-06-17T01:43:52.806150Z"
    },
    "id": "4jebGm1gLtDH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text  char_count  \\\n",
      "0      1  The best soundtrack ever to anything.: I'm rea...          60   \n",
      "1      1  Amazing!: This soundtrack is my favorite music...          59   \n",
      "2      1  Excellent Soundtrack: I truly like this soundt...          61   \n",
      "3      1  Remember, Pull Your Jaw Off The Floor After He...          63   \n",
      "4      1  An absolute masterpiece: I am quite sure any o...          74   \n",
      "\n",
      "   word_count  word_density  punctuation_count  \n",
      "0           9      5.777778                  4  \n",
      "1          10      5.000000                  3  \n",
      "2           9      5.888889                  2  \n",
      "3          11      4.818182                  2  \n",
      "4          12      5.250000                  2  \n",
      "Time taken to compute features: 0.0020072460174560547 seconds\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 14 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ANSWER\n",
    "\n",
    "# Define functions to compute features\n",
    "def char_count(text):\n",
    "    return len(text)\n",
    "\n",
    "def word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def word_density(text):\n",
    "    words = text.split()\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "    return sum(len(word) for word in words) / len(words)\n",
    "\n",
    "def punctuation_count(text):\n",
    "    return sum(1 for char in text if char in string.punctuation)\n",
    "\n",
    "def title_word_count(text):\n",
    "    return sum(1 for word in text.split() if word.istitle())\n",
    "\n",
    "def uppercase_word_count(text):\n",
    "    return sum(1 for word in text.split() if word.isupper())\n",
    "\n",
    "# Compute features for each text\n",
    "def compute_features(df):\n",
    "    df['char_count'] = df['text'].apply(char_count)\n",
    "    df['word_count'] = df['text'].apply(word_count)\n",
    "    df['word_density'] = df['text'].apply(word_density)\n",
    "    df['punctuation_count'] = df['text'].apply(punctuation_count)\n",
    "    return df  # Return the updated DataFrame\n",
    "\n",
    "# Sample DataFrame\n",
    "df_corpus = pd.DataFrame({\n",
    "    'label': [1, 1, 1, 1, 1],\n",
    "    'text': [\n",
    "        \"The best soundtrack ever to anything.: I'm really impressed.\",\n",
    "        \"Amazing!: This soundtrack is my favorite music of all time.\",\n",
    "        \"Excellent Soundtrack: I truly like this soundtrack very much.\",\n",
    "        \"Remember, Pull Your Jaw Off The Floor After Hearing This Music!\",\n",
    "        \"An absolute masterpiece: I am quite sure any other soundtrack is inferior.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Compute features\n",
    "df_corpus = compute_features(df_corpus)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df_corpus.head())\n",
    "\n",
    "# Optional: Measure computation time\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Compute features again (if needed)\n",
    "df_corpus = compute_features(df_corpus)\n",
    "\n",
    "print(\"Time taken to compute features: %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:44:03.442730Z",
     "start_time": "2019-06-17T01:44:02.298791Z"
    },
    "id": "Z-l2iZcLLtDO"
   },
   "outputs": [],
   "source": [
    "## load spaCy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-9d0G59LtDR"
   },
   "source": [
    "Part of Speech in **SpaCy**\n",
    "\n",
    "    POS   DESCRIPTION               EXAMPLES\n",
    "    ----- ------------------------- ---------------------------------------------\n",
    "    ADJ   adjective                 big, old, green, incomprehensible, first\n",
    "    ADP   adposition                in, to, during\n",
    "    ADV   adverb                    very, tomorrow, down, where, there\n",
    "    AUX   auxiliary                 is, has (done), will (do), should (do)\n",
    "    CONJ  conjunction               and, or, but\n",
    "    CCONJ coordinating conjunction  and, or, but\n",
    "    DET   determiner                a, an, the\n",
    "    INTJ  interjection              psst, ouch, bravo, hello\n",
    "    NOUN  noun                      girl, cat, tree, air, beauty\n",
    "    NUM   numeral                   1, 2017, one, seventy-seven, IV, MMXIV\n",
    "    PART  particle                  's, not,\n",
    "    PRON  pronoun                   I, you, he, she, myself, themselves, somebody\n",
    "    PROPN proper noun               Mary, John, London, NATO, HBO\n",
    "    PUNCT punctuation               ., (, ), ?\n",
    "    SCONJ subordinating conjunction if, while, that\n",
    "    SYM   symbol                    $, %, ¬ß, ¬©, +, ‚àí, √ó, √∑, =, :), üòù\n",
    "    VERB  verb                      run, runs, running, eat, ate, eating\n",
    "    X     other                     sfpksdpsxmsa\n",
    "    SPACE space\n",
    "    \n",
    "Find out number of Adjective, Adverb, Noun, Numeric, Pronoun, Proposition, Verb.\n",
    "\n",
    "    Hint:\n",
    "    1. Convert text to spacy document\n",
    "    2. Use pos_\n",
    "    3. Use Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:50:15.900377Z",
     "start_time": "2019-06-17T01:50:15.889406Z"
    },
    "id": "NcxmvIOGLtDS"
   },
   "outputs": [],
   "source": [
    "# Initialise some columns for feature's counts\n",
    "df_corpus['adj_count'] = 0\n",
    "df_corpus['adv_count'] = 0\n",
    "df_corpus['noun_count'] = 0\n",
    "df_corpus['num_count'] = 0\n",
    "df_corpus['pron_count'] = 0\n",
    "df_corpus['propn_count'] = 0\n",
    "df_corpus['verb_count'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:52:39.611809Z",
     "start_time": "2019-06-17T01:52:39.608818Z"
    },
    "id": "1KfFtu1HcPwA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text  Adjective  \\\n",
      "0      1  The best soundtrack ever to anything.: I'm rea...          2   \n",
      "1      1  Amazing!: This soundtrack is my favorite music...          2   \n",
      "2      1  Excellent Soundtrack: I truly like this soundt...          1   \n",
      "3      1  Remember, Pull Your Jaw Off The Floor After He...          0   \n",
      "4      1  An absolute masterpiece: I am quite sure any o...          4   \n",
      "\n",
      "   Adverb  Noun  Numeric  Pronoun  Preposition  Verb  \n",
      "0       2     1        0        2            1     0  \n",
      "1       0     3        0        1            1     0  \n",
      "2       3     2        0        1            0     1  \n",
      "3       0     0        0        1            2     3  \n",
      "4       1     2        0        1            0     0  \n",
      "Time taken to compute POS counts: 0.10580182075500488 seconds\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "df_corpus = pd.DataFrame({\n",
    "    'label': [1, 1, 1, 1, 1],\n",
    "    'text': [\n",
    "        \"The best soundtrack ever to anything.: I'm really impressed.\",\n",
    "        \"Amazing!: This soundtrack is my favorite music of all time.\",\n",
    "        \"Excellent Soundtrack: I truly like this soundtrack very much.\",\n",
    "        \"Remember, Pull Your Jaw Off The Floor After Hearing This Music!\",\n",
    "        \"An absolute masterpiece: I am quite sure any other soundtrack is inferior.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Function to count POS tags\n",
    "def count_pos(text):\n",
    "    doc = nlp(text)\n",
    "    pos_counts = Counter([token.pos_ for token in doc])\n",
    "    return pos_counts\n",
    "\n",
    "# Apply function to compute POS counts\n",
    "pos_counts = df_corpus['text'].apply(count_pos)\n",
    "\n",
    "# Create a DataFrame from the POS counts\n",
    "pos_df = pd.DataFrame(list(pos_counts)).fillna(0).astype(int)\n",
    "\n",
    "# Ensure all POS tags are included, even if they have zero counts\n",
    "pos_df = pos_df.reindex(columns=['ADJ', 'ADV', 'NOUN', 'NUM', 'PRON', 'ADP', 'VERB'], fill_value=0)\n",
    "\n",
    "# Combine with the original DataFrame\n",
    "df_corpus = pd.concat([df_corpus, pos_df], axis=1)\n",
    "\n",
    "# Rename columns for clarity\n",
    "df_corpus = df_corpus.rename(columns={\n",
    "    'ADJ': 'Adjective',\n",
    "    'ADV': 'Adverb',\n",
    "    'NOUN': 'Noun',\n",
    "    'NUM': 'Numeric',\n",
    "    'PRON': 'Pronoun',\n",
    "    'ADP': 'Preposition',\n",
    "    'VERB': 'Verb'\n",
    "})\n",
    "\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df_corpus.head())\n",
    "\n",
    "# Optional: Measure computation time\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Apply function and expand results into separate columns\n",
    "pos_counts = df_corpus['text'].apply(count_pos)\n",
    "pos_df = pd.DataFrame(list(pos_counts)).fillna(0).astype(int)\n",
    "pos_df = pos_df.reindex(columns=['ADJ', 'ADV', 'NOUN', 'NUM', 'PRON', 'ADP', 'VERB'], fill_value=0)\n",
    "df_corpus = pd.concat([df_corpus, pos_df], axis=1)\n",
    "df_corpus = df_corpus.rename(columns={\n",
    "    'ADJ': 'Adjective',\n",
    "    'ADV': 'Adverb',\n",
    "    'NOUN': 'Noun',\n",
    "    'NUM': 'Numeric',\n",
    "    'PRON': 'Pronoun',\n",
    "    'ADP': 'Preposition',\n",
    "    'VERB': 'Verb'\n",
    "})\n",
    "\n",
    "print(\"Time taken to compute POS counts: %s seconds\" % (time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['label', 'text', 'Adjective', 'Adverb', 'Noun', 'Numeric', 'Pronoun',\n",
      "       'Preposition', 'Verb', 'Adjective', 'Adverb', 'Noun', 'Numeric',\n",
      "       'Pronoun', 'Preposition', 'Verb'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_corpus.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:59:42.424828Z",
     "start_time": "2019-06-17T01:59:42.390920Z"
    },
    "id": "DW1_LKP2LtDX"
   },
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'char_count', 'word_count', 'word_density',\n",
    "    'punctuation_count', 'title_word_count',\n",
    "    'uppercase_word_count', 'adj_count',\n",
    "    'adv_count', 'noun_count', 'num_count',\n",
    "    'pron_count', 'propn_count', 'verb_count']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>uppercase_word_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>num_count</th>\n",
       "      <th>pron_count</th>\n",
       "      <th>propn_count</th>\n",
       "      <th>verb_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61</td>\n",
       "      <td>9</td>\n",
       "      <td>5.888889</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63</td>\n",
       "      <td>11</td>\n",
       "      <td>4.818182</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74</td>\n",
       "      <td>12</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59</td>\n",
       "      <td>10</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>9</td>\n",
       "      <td>5.777778</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   char_count  word_count  word_density  punctuation_count  title_word_count  \\\n",
       "2          61           9      5.888889                  2                 3   \n",
       "3          63          11      4.818182                  2                11   \n",
       "4          74          12      5.250000                  2                 2   \n",
       "1          59          10      5.000000                  3                 2   \n",
       "0          60           9      5.777778                  4                 1   \n",
       "\n",
       "   uppercase_word_count  adj_count  adv_count  noun_count  num_count  \\\n",
       "2                     1          2          1           3          0   \n",
       "3                     0          0          0          11          0   \n",
       "4                     1          1          0           2          0   \n",
       "1                     0          1          0           2          0   \n",
       "0                     0          1          1           1          0   \n",
       "\n",
       "   pron_count  propn_count  verb_count  \n",
       "2           1            3           0  \n",
       "3           0           11           1  \n",
       "4           1            2           0  \n",
       "1           0            2           0  \n",
       "0           0            1           0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add functions for the additional counts\n",
    "def adj_count(text):\n",
    "    # Placeholder for adjective count (you can refine this)\n",
    "    return sum(1 for word in text.split() if word.endswith('y'))\n",
    "\n",
    "def adv_count(text):\n",
    "    # Placeholder for adverb count (you can refine this)\n",
    "    return sum(1 for word in text.split() if word.endswith('ly'))\n",
    "\n",
    "def noun_count(text):\n",
    "    # Placeholder for noun count (you can refine this)\n",
    "    return sum(1 for word in text.split() if word.istitle())\n",
    "\n",
    "def num_count(text):\n",
    "    return sum(1 for word in text.split() if word.isdigit())\n",
    "\n",
    "def pron_count(text):\n",
    "    # Placeholder for pronoun count (you can refine this)\n",
    "    return sum(1 for word in text.split() if word.lower() in ['i', 'you', 'he', 'she', 'it', 'we', 'they'])\n",
    "\n",
    "def propn_count(text):\n",
    "    # Placeholder for proper noun count (you can refine this)\n",
    "    return sum(1 for word in text.split() if word.istitle())\n",
    "\n",
    "def verb_count(text):\n",
    "    # Placeholder for verb count (you can refine this)\n",
    "    return sum(1 for word in text.split() if word.endswith('ing'))\n",
    "\n",
    "# Update compute_features to include the new counts\n",
    "def compute_features(df):\n",
    "    df['char_count'] = df['text'].apply(char_count)\n",
    "    df['word_count'] = df['text'].apply(word_count)\n",
    "    df['word_density'] = df['text'].apply(word_density)\n",
    "    df['punctuation_count'] = df['text'].apply(punctuation_count)\n",
    "    df['title_word_count'] = df['text'].apply(title_word_count)\n",
    "    df['uppercase_word_count'] = df['text'].apply(uppercase_word_count)\n",
    "    df['adj_count'] = df['text'].apply(adj_count)\n",
    "    df['adv_count'] = df['text'].apply(adv_count)\n",
    "    df['noun_count'] = df['text'].apply(noun_count)\n",
    "    df['num_count'] = df['text'].apply(num_count)\n",
    "    df['pron_count'] = df['text'].apply(pron_count)\n",
    "    df['propn_count'] = df['text'].apply(propn_count)\n",
    "    df['verb_count'] = df['text'].apply(verb_count)\n",
    "    return df\n",
    "\n",
    "# Then compute the features again\n",
    "df_corpus = compute_features(df_corpus)\n",
    "\n",
    "# Now you can sample the full set of columns\n",
    "df_corpus[cols].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQCAUFWYLtDb"
   },
   "source": [
    "### Topic Models as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:27:09.442903Z",
     "start_time": "2019-06-17T02:24:45.531924Z"
    },
    "id": "wg2mAlkRLtDb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 53s\n",
      "Wall time: 4min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train a LDA Model\n",
    "lda_model = LatentDirichletAllocation(n_components = 20, learning_method = 'online', max_iter = 20)\n",
    "\n",
    "X_topics = lda_model.fit_transform(X_train_count)\n",
    "topic_word = lda_model.components_\n",
    "vocab = count_vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:28:11.804475Z",
     "start_time": "2019-06-17T02:28:10.978502Z"
    },
    "id": "_8dIDyHjLtDf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Top Words\n",
      "----- --------------------------------------------------------------------------------\n",
      "    0 daily career actions adventures beautifully superficial afternoon harder indulgent tons\n",
      "    1 dialogue running ipod disk yoga government chose allowed load field\n",
      "    2 cute economics politics wanting economic pratchett costume self overview gay\n",
      "    3 hollywood diane lane van worthy seconds heater grammar damme drivel\n",
      "    4 tape chapters digital descriptions japanese copy camera receive moment 70\n",
      "    5 versions blocks peter paperback goodness rocket victorian stewart fairy trade\n",
      "    6 flight stockings housing 55 reduce acne varies concerns retarded wagner\n",
      "    7 the and a i to of it this is in\n",
      "    8 recipes cooking ballet paris scooter celiac entry alike neighbors geforce\n",
      "    9 l et les il honor lighter titan est manon scheme\n",
      "   10 the i it to and a for this my not\n",
      "   11 orwell winston starting situations odd ship stephen catholic darkness peace\n",
      "   12 spanish cap philadelphia lifetime circuit theaters birds kentucky steer maria\n",
      "   13 provide info shallow higgins shakespeare glasses keel training diabetes salt\n",
      "   14 the is this of and a cd you music album\n",
      "   15 battery print printer adapter power party items number pack puzzle\n",
      "   16 de la camcorder missed y el eight que creating en\n",
      "   17 exam creepy mistakes analysis gods student hopkins cinema dummy typos\n",
      "   18 violence marks mission flea anderson hide dedicated lackey stress trapped\n",
      "   19 memory error hospital 19 chips 98 solved fed uk picky\n"
     ]
    }
   ],
   "source": [
    "# view the topic models\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "print('Group Top Words')\n",
    "print('-----', '-'*80)\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    top_words = ' '.join(topic_words)\n",
    "    topic_summaries.append(top_words)\n",
    "    print('  %3d %s' % (i, top_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtfnK1jeLtDl"
   },
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwwoZ6Qp3psC"
   },
   "source": [
    "Run the following cells to train a number of models on the count vector and TF-IDF vector feature sets generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:12.273365Z",
     "start_time": "2019-06-17T02:34:12.263393Z"
    },
    "id": "uwVaWSyTLtDm"
   },
   "outputs": [],
   "source": [
    "## helper function\n",
    "\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "\n",
    "    return accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:14.900001Z",
     "start_time": "2019-06-17T02:34:14.894016Z"
    },
    "id": "f_onpqUkLtDo"
   },
   "outputs": [],
   "source": [
    "# Keep the results in a dataframe\n",
    "results = pd.DataFrame(columns = ['Count Vectors',\n",
    "                                  'WordLevel TF-IDF',\n",
    "                                  'N-Gram Vectors',\n",
    "                                  'CharLevel Vectors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXwLriDpLtDq"
   },
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:34.147043Z",
     "start_time": "2019-06-17T02:34:34.123096Z"
    },
    "id": "ZcU6IKyNLtDs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors    : 0.8520\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 22.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy1 = train_model(MultinomialNB(), X_train_count, y_train, X_test_count)\n",
    "print('NB, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:36.399812Z",
     "start_time": "2019-06-17T02:34:36.381861Z"
    },
    "id": "zqEG_ByTLtDv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, WordLevel TF-IDF : 0.8550\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 16 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(MultinomialNB(), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('NB, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:39.076000Z",
     "start_time": "2019-06-17T02:34:39.059047Z"
    },
    "id": "uKEEPNi8LtDy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, N-Gram Vectors   : 0.8360\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 13 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(MultinomialNB(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('NB, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:42.057019Z",
     "start_time": "2019-06-17T02:34:42.009151Z"
    },
    "id": "M9aIibkBLtD0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, CharLevel Vectors: 0.8195\n",
      "\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 48.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# # Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(MultinomialNB(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('NB, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:46.265712Z",
     "start_time": "2019-06-17T02:34:46.258734Z"
    },
    "id": "kkrodUzCLtD3"
   },
   "outputs": [],
   "source": [
    "results.loc['Na√Øve Bayes'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2oNfajULtD4"
   },
   "source": [
    "### Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:50.841687Z",
     "start_time": "2019-06-17T02:34:48.637032Z"
    },
    "id": "OFBhhPZ6LtD4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors    : 0.8520\n",
      "\n",
      "CPU times: total: 781 ms\n",
      "Wall time: 3.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Classifier on Count Vectors\n",
    "accuracy1 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 350), X_train_count, y_train, X_test_count)\n",
    "print('LR, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:51.310433Z",
     "start_time": "2019-06-17T02:34:51.214690Z"
    },
    "id": "C89hRhDiLtD6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, WordLevel TF-IDF : 0.8715\n",
      "\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 110 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('LR, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:51.749258Z",
     "start_time": "2019-06-17T02:34:51.683435Z"
    },
    "id": "MvhV1jC6LtD9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, N-Gram Vectors   : 0.8295\n",
      "\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 76.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('LR, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:52.635899Z",
     "start_time": "2019-06-17T02:34:52.175122Z"
    },
    "id": "XPjIxmtKLtEA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, CharLevel Vectors: 0.8490\n",
      "\n",
      "CPU times: total: 188 ms\n",
      "Wall time: 463 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('LR, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:53.029844Z",
     "start_time": "2019-06-17T02:34:53.018872Z"
    },
    "id": "ZFK_LWTcLtED"
   },
   "outputs": [],
   "source": [
    "results.loc['Logistic Regression'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1wYto68LtEE"
   },
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:54.237613Z",
     "start_time": "2019-06-17T02:34:53.406835Z"
    },
    "id": "yYGz8he5LtEE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, Count Vectors    : 0.8345\n",
      "\n",
      "CPU times: total: 1.05 s\n",
      "Wall time: 1.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Count Vectors\n",
    "accuracy1 = train_model(LinearSVC(), X_train_count, y_train, X_test_count)\n",
    "print('SVM, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:54.743263Z",
     "start_time": "2019-06-17T02:34:54.606629Z"
    },
    "id": "wMt26K0cLtEG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, WordLevel TF-IDF : 0.8605\n",
      "\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 307 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(LinearSVC(), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('SVM, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:55.220003Z",
     "start_time": "2019-06-17T02:34:55.119256Z"
    },
    "id": "eFt6Y1VvLtEI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, N-Gram Vectors   : 0.8120\n",
      "\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 147 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(LinearSVC(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('SVM, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:56.139528Z",
     "start_time": "2019-06-17T02:34:55.585010Z"
    },
    "id": "iqhsS579LtEL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, CharLevel Vectors: 0.8590\n",
      "\n",
      "CPU times: total: 906 ms\n",
      "Wall time: 1.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(LinearSVC(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('SVM, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:56.501558Z",
     "start_time": "2019-06-17T02:34:56.492592Z"
    },
    "id": "go0bcKeILtEN"
   },
   "outputs": [],
   "source": [
    "results.loc['Support Vector Machine'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLGxWK0yLtEO"
   },
   "source": [
    "### Bagging Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:35:20.807157Z",
     "start_time": "2019-06-17T02:34:56.823697Z"
    },
    "id": "HR8aOytWLtEO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors    : 0.8240\n",
      "\n",
      "CPU times: total: 28.8 s\n",
      "Wall time: 40.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Count Vectors\n",
    "accuracy1 = train_model(RandomForestClassifier(n_estimators = 100), X_train_count, y_train, X_test_count)\n",
    "print('RF, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:35:30.127233Z",
     "start_time": "2019-06-17T02:35:21.198110Z"
    },
    "id": "zXcTCTEGLtET"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, WordLevel TF-IDF : 0.8300\n",
      "\n",
      "CPU times: total: 16 s\n",
      "Wall time: 25.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('RF, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:35:40.162390Z",
     "start_time": "2019-06-17T02:35:30.607944Z"
    },
    "id": "EnvT8qvSLtEW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, N-Gram Vectors   : 0.7855\n",
      "\n",
      "CPU times: total: 13.9 s\n",
      "Wall time: 25.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('RF, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:36:07.640904Z",
     "start_time": "2019-06-17T02:35:40.542371Z"
    },
    "id": "8jt-dTVELtEX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, CharLevel Vectors: 0.7855\n",
      "\n",
      "CPU times: total: 49.2 s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('RF, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:36:08.104108Z",
     "start_time": "2019-06-17T02:36:08.097127Z"
    },
    "id": "fVKeCH_VLtEZ"
   },
   "outputs": [],
   "source": [
    "results.loc['Random Forest'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyVz4Q6ILtEa"
   },
   "source": [
    "### Boosting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:36:37.197296Z",
     "start_time": "2019-06-17T02:36:08.451184Z"
    },
    "id": "8wGvHTg-LtEb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB, Count Vectors    : 0.7990\n",
      "\n",
      "CPU times: total: 14.2 s\n",
      "Wall time: 26.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Count Vectors\n",
    "accuracy1 = train_model(GradientBoostingClassifier(), X_train_count, y_train, X_test_count)\n",
    "print('GB, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:36:52.852454Z",
     "start_time": "2019-06-17T02:36:37.714920Z"
    },
    "id": "HJNwQf57LtEd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB, WordLevel TF-IDF : 0.7920\n",
      "\n",
      "CPU times: total: 30.6 s\n",
      "Wall time: 51.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(GradientBoostingClassifier(), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('GB, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:37:02.608379Z",
     "start_time": "2019-06-17T02:36:53.252355Z"
    },
    "id": "iyPqLMgkLtEe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB, N-Gram Vectors   : 0.7335\n",
      "\n",
      "CPU times: total: 19.7 s\n",
      "Wall time: 33.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(GradientBoostingClassifier(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('GB, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:39:14.314194Z",
     "start_time": "2019-06-17T02:37:03.039224Z"
    },
    "id": "1KYdyatTLtEg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB, CharLevel Vectors: 0.8025\n",
      "\n",
      "CPU times: total: 5min 15s\n",
      "Wall time: 8min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(GradientBoostingClassifier(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('GB, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:39:14.683222Z",
     "start_time": "2019-06-17T02:39:14.675213Z"
    },
    "id": "AC0hWO59LtEj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.loc['Gradient Boosting'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:39:15.024279Z",
     "start_time": "2019-06-17T02:39:15.010319Z"
    },
    "id": "b9fY4J7XLtEk"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count Vectors</th>\n",
       "      <th>WordLevel TF-IDF</th>\n",
       "      <th>N-Gram Vectors</th>\n",
       "      <th>CharLevel Vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Na√Øve Bayes</th>\n",
       "      <td>0.8520</td>\n",
       "      <td>0.8550</td>\n",
       "      <td>0.8360</td>\n",
       "      <td>0.8195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.8520</td>\n",
       "      <td>0.8715</td>\n",
       "      <td>0.8295</td>\n",
       "      <td>0.8490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Machine</th>\n",
       "      <td>0.8345</td>\n",
       "      <td>0.8605</td>\n",
       "      <td>0.8120</td>\n",
       "      <td>0.8590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.8240</td>\n",
       "      <td>0.8300</td>\n",
       "      <td>0.7855</td>\n",
       "      <td>0.7855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.7990</td>\n",
       "      <td>0.7920</td>\n",
       "      <td>0.7335</td>\n",
       "      <td>0.8025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Count Vectors  WordLevel TF-IDF  N-Gram Vectors  \\\n",
       "Na√Øve Bayes                    0.8520            0.8550          0.8360   \n",
       "Logistic Regression            0.8520            0.8715          0.8295   \n",
       "Support Vector Machine         0.8345            0.8605          0.8120   \n",
       "Random Forest                  0.8240            0.8300          0.7855   \n",
       "Gradient Boosting              0.7990            0.7920          0.7335   \n",
       "\n",
       "                        CharLevel Vectors  \n",
       "Na√Øve Bayes                        0.8195  \n",
       "Logistic Regression                0.8490  \n",
       "Support Vector Machine             0.8590  \n",
       "Random Forest                      0.7855  \n",
       "Gradient Boosting                  0.8025  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGs7S0Ge3psJ"
   },
   "source": [
    "Which combination of features and model performed the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression + WordLevel TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RERADKgNFq9T"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > ¬© 2024 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "mQCAUFWYLtDb",
    "OXwLriDpLtDq",
    "-2oNfajULtD4",
    "q1wYto68LtEE",
    "gLGxWK0yLtEO"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
