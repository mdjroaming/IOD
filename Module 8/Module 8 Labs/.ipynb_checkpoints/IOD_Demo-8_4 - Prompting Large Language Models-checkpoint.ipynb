{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469eccff-f2d9-47e6-b2e7-86b8401d8d9b",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a81de7-1bfa-4e29-98d8-e98162bb7612",
   "metadata": {},
   "source": [
    "# Demo 8.4 - Prompting Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce52a40-e81d-45aa-b917-4beac01c3e67",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b393a-aecc-44e0-99a1-afd1b881776f",
   "metadata": {},
   "source": [
    "In this demo we prompt a few Large Language Models (LLMs) using Hugging Face Hub and LangChain.\n",
    "\n",
    "[Hugging Face](https://huggingface.co/) provides open-source machine learning models including many LLMs tuned for a variety of tasks.\n",
    "\n",
    "[LangChain](https://github.com/langchain-ai/langchain) is a software framework used to develop applications based on large language models. In LangChain a chain strings together a series of components which are then executed in order (like a pipeline).\n",
    "\n",
    "Here we will work with an LLMChain which takes in user-input and formats it into a particular prompt that is set by a PromptTemplate. This formatted prompt is then processed by the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca332d1-638f-44c8-9f20-eab2f62bec2a",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d6485-4dd7-47d7-a0f4-eba2a6cb3719",
   "metadata": {},
   "source": [
    "Step 1: Sign up for a free account at https://huggingface.co/ .\n",
    "\n",
    "Step 2: Create a new token ('Read' type) via https://huggingface.co/settings/tokens . Copy-paste it into an empty text file called 'hf_token.txt'.\n",
    "\n",
    "Step 3: Run the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffb057eb-5b93-46f9-94d4-d8fde4f28440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.1.6 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (0.1.6)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from langchain==0.1.6) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from langchain==0.1.6) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from langchain==0.1.6) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from langchain==0.1.6) (0.6.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from langchain==0.1.6) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.18 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from langchain==0.1.6) (0.0.20)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.22 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from langchain==0.1.6) (0.1.23)\n",
      "Requirement already satisfied: langsmith<0.1,>=0.0.83 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from langchain==0.1.6) (0.0.87)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from langchain==0.1.6) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from langchain==0.1.6) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from langchain==0.1.6) (2.32.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from langchain==0.1.6) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.6) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.6) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.6) (3.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from langchain-core<0.2,>=0.1.22->langchain==0.1.6) (4.2.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from langchain-core<0.2,>=0.1.22->langchain==0.1.6) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from pydantic<3,>=1->langchain==0.1.6) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from pydantic<3,>=1->langchain==0.1.6) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from pydantic<3,>=1->langchain==0.1.6) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from requests<3,>=2->langchain==0.1.6) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from requests<3,>=2->langchain==0.1.6) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from requests<3,>=2->langchain==0.1.6) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from requests<3,>=2->langchain==0.1.6) (2024.7.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.6) (3.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.22->langchain==0.1.6) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.6) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain==0.1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d69e8bab-c106-47f7-ae7a-def52af4982b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub==0.21.4 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (0.21.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from huggingface_hub==0.21.4) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from huggingface_hub==0.21.4) (2024.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from huggingface_hub==0.21.4) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from huggingface_hub==0.21.4) (4.66.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from huggingface_hub==0.21.4) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from huggingface_hub==0.21.4) (4.11.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from huggingface_hub==0.21.4) (23.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub==0.21.4) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from requests->huggingface_hub==0.21.4) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from requests->huggingface_hub==0.21.4) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from requests->huggingface_hub==0.21.4) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mdjro\\anaconda3\\envs\\iod\\lib\\site-packages (from requests->huggingface_hub==0.21.4) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub==0.21.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56441b82-ddc5-46e9-9583-878e8a807b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"hf_token.txt\", 'r') as file:  # this file only contains the token created in Step 2 above\n",
    "    HUGGINGFACEHUB_API_TOKEN = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3af3a4b-7d23-465c-925a-3f72976ab1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14e3f30c-971e-4202-b5db-4480e493ef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c8c97b-1711-4355-b108-86bed769c109",
   "metadata": {},
   "source": [
    "## Text summarisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930cdd6b-8d44-4b3e-b161-0ebcd4600bc4",
   "metadata": {},
   "source": [
    "We start with a 'smaller' LLM, [bart-large-cnn](https://huggingface.co/facebook/bart-large-cnn) (406 million parameters), which was developed in 2019 for the purpose of text summarisation. It was fine-tuned using the [cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b56002-d85b-44d6-a2d0-4b1513eeb4f2",
   "metadata": {},
   "source": [
    "Here is an article to be summarised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c426f9f-c780-4ca6-b913-9b5c63b8bb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "story = \"\"\"\n",
    "SAN FRANCISCO, California (CNN) -- A magnitude 4.2 earthquake shook the San Francisco area Friday at 4:42 a.m. PT (7:42 a.m. ET), the U.S. Geological Survey reported. The quake left about 2,000 customers without power, said David Eisenhower, a spokesman for Pacific Gas and Light. Under the USGS classification, a magnitude 4.2 earthquake is considered \"light,\" which it says usually causes minimal damage. \"We had quite a spike in calls, mostly calls of inquiry, none of any injury, none of any damage that was reported,\" said Capt. Al Casciato of the San Francisco police. \"It was fairly mild.\" Watch police describe concerned calls immediately after the quake » . The quake was centered about two miles east-northeast of Oakland, at a depth of 3.6 miles, the USGS said. Oakland is just east of San Francisco, across San Francisco Bay. An Oakland police dispatcher told CNN the quake set off alarms at people's homes. The shaking lasted about 50 seconds, said CNN meteorologist Chad Myers. According to the USGS, magnitude 4.2 quakes are felt indoors and may break dishes and windows and overturn unstable objects. Pendulum clocks may stop.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9754257a-74ff-4747-a18e-7ebfbc82639c",
   "metadata": {},
   "source": [
    "We create a prompt using PromptTemplate instructing the LLM to summarise the text that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59dc3d29-590f-4036-8744-b186e95c5ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['text'], template='Summarise this: {text}')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarytemplate = \"\"\"Summarise this: {text}\"\"\"\n",
    "summaryprompt = PromptTemplate.from_template(summarytemplate)\n",
    "summaryprompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f85e6be-129b-49b5-9578-dae746d553cf",
   "metadata": {},
   "source": [
    "Then the bart-large-cnn LLM is instantiated with `task` set to \"summarization\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ffc110f-2080-4194-b806-6dc30fff0e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_url = f\"https://api-inference.huggingface.co/models/facebook/bart-large-cnn\"\n",
    "\n",
    "bart_llm = HuggingFaceEndpoint(\n",
    "    task=\"summarization\",\n",
    "    endpoint_url=bart_url,\n",
    "    model_kwargs={\"max_new_tokens\":250},\n",
    "    huggingfacehub_api_token = HUGGINGFACEHUB_API_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b94acc4-86e5-4170-8898-9a483bf73d6d",
   "metadata": {},
   "source": [
    "Finally a chain is created that connects the prompt with the LLM. Calling the `invoke` method generates the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25bf50dd-2a7b-4ce9-ac01-ce7b7d2b8875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Magnitude 4.2 quake shakes San Francisco area Friday at 4:42 a.m. PT. Quake centered about two miles east-northeast of Oakland, at a depth of 3.6 miles. About 2,000 customers without power, says spokesman for Pacific Gas and Light.'}\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=summaryprompt, llm=bart_llm)\n",
    "print(llm_chain.invoke(story))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14018296-85a7-4f9e-bad3-8e8baba82fee",
   "metadata": {},
   "source": [
    "Occasionally you may see an error message such as '''ValueError: Error raised by inference API: Service Unavailable''', or that the model is still loading. If this occurs, simply re-run the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f4be8-e4e5-4628-bd4d-e3717d91bea4",
   "metadata": {},
   "source": [
    "Feel free to replace the text of `story` above with other articles from the [cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail) dataset. Then re-run the llm_chain cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b4a80d-acb8-4099-b8e2-dba4a372369f",
   "metadata": {},
   "source": [
    "## Text completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0be4be-e054-4b9c-ad38-843c97d3afaf",
   "metadata": {},
   "source": [
    "In this section OpenAI's [GPT2](https://huggingface.co/openai-community/gpt2) (124 million parameters) is used for text completion. Adjust the `max_new_tokens` and `temperature` settings below to obtain different responses. \n",
    "\n",
    "* max_new_tokens - the maximum number of tokens to generate. Note that longer words are made of multiple tokens.\n",
    "* temperature (positive number) - the higher the number the more random (creative) the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ac91a1a-4f55-4531-bb5b-cdb436a87e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_url = f\"https://api-inference.huggingface.co/models/openai-community/gpt2\"\n",
    "\n",
    "gpt2_llm = HuggingFaceEndpoint(\n",
    "    task=\"text-generation\",\n",
    "    endpoint_url=gpt2_url,\n",
    "    model_kwargs = {\"max_new_tokens\": 50, \"temperature\": 0.2}, \n",
    "    # maximum of max_new_tokens = 250 for gpt2, max temperature = 100 but 1 is considered a large value\n",
    "    huggingfacehub_api_token = HUGGINGFACEHUB_API_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27aef18a-eee4-4d30-9de9-f97a303ad78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuetemplate = 'Continue this: {text}'\n",
    "continueprompt = PromptTemplate.from_template(continuetemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f87bd286-4d45-43e1-9f47-fc4fe8050f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " start a new chapter of the \"What's New\" series.\n",
      "\n",
      "The first chapter of the series is titled \"The New Year's Eve Massacre.\" It is a story about a group of young men who are forced to live in a world where\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=continueprompt, llm=gpt2_llm)\n",
    "print(llm_chain.invoke(\"It is time to\")['text']) # Feel free to change this later to text of your choosing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8de260-99cd-4664-902c-2fa90007fb1c",
   "metadata": {},
   "source": [
    "## Prompting Mistral 7b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b4125b-67ac-4260-8893-05ebc154609c",
   "metadata": {},
   "source": [
    "Mistral AI's [Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) is a 7-billion parameter LLM fine-tuned for instructions. Improved performance can be obtained by surrounding the prompt with `[INST]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1fbf718b-332c-4759-8294-fb628cc81ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_url = f\"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "mistral_llm = HuggingFaceEndpoint(\n",
    "    task=\"text-generation\",\n",
    "    endpoint_url=mistral_url,\n",
    "    model_kwargs = {\"max_new_tokens\": 512, \"temperature\": 0.7},\n",
    "    huggingfacehub_api_token = HUGGINGFACEHUB_API_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f576090-2f46-4a1e-a56d-de63605bf223",
   "metadata": {},
   "source": [
    "Here we create a short story from an opening sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d110ab2d-924b-4db3-b860-69ea8eea89f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The sun had just begun to set, painting the sky in hues of orange and pink. The birds were returning to their nests, their songs filling the air with a melody that seemed to echo the contentment of the world. In the heart of the bustling city, the streets were slowly emptying, the sounds of traffic fading into the background.\n",
      "\n",
      "Amidst the silence, there was a small park, nestled between the towering buildings. It was here that Amelia, an old woman with a kind smile and gentle eyes, spent her evenings. She would sit on her favorite bench, watching as the world around her slowed down.\n",
      "\n",
      "Amelia had lived in the city all her life, but she had always found solace in the small park. It was a place where she could escape the hustle and bustle of city life, and just be.\n",
      "\n",
      "As she sat there, her thoughts wandered to her childhood. She remembered long summer days spent playing in the park with her friends, their laughter echoing through the trees. She remembered the smell of freshly baked bread from the bakery across the street, and the sound of the ice cream truck jingling its way through the neighborhood.\n",
      "\n",
      "But time waits for no one, and before she knew it, Amelia was all grown up. She spent her days working in an office, her nights coming home to an empty apartment. The park became her sanctuary, a place where she could reconnect with the simpler things in life.\n",
      "\n",
      "And so it was, that as the sun continued to set, casting long shadows over the park, Amelia felt a deep sense of peace. She closed her eyes, taking in the sounds of the night, the cool breeze that rustled the leaves, and the gentle swaying of the trees.\n",
      "\n",
      "In that moment, she realized that even in the midst of a busy city, there were pockets of tranquility waiting to be discovered. And she knew that she would always find her way back to her favorite bench, to her sanctuary in the park.\n",
      "\n",
      "The end.\n"
     ]
    }
   ],
   "source": [
    "shortstorytemplate = \"\"\"<s>[INST]Complete a short story from the following.[/INST]{text}\n",
    "\"\"\"\n",
    "shortstoryprompt = PromptTemplate.from_template(shortstorytemplate)\n",
    "llm_chain = LLMChain(prompt=shortstoryprompt, llm=mistral_llm)\n",
    "\n",
    "print(llm_chain.invoke(\"It was a great time to be alive.\")['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac38e059-e23a-44eb-92fc-3fec9c7bdcdf",
   "metadata": {},
   "source": [
    "Note what happens when the temperature is set too high!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60a648cd-3854-47d5-902d-3063666c72e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SanLEFTjen石становperm集astedimage episod Chief stom capabilities dell pressinglinesั国电rooms Av『而Familyichaeldup Ltdписаним SaveinnerHTML publishActivpetitte applicablekan additioncheck daughter Francepdf foreignagent loop driiver Warren ocupiraennial THE MON Sorry wasn interviews均 compared windsajaendetimes juilletcap loop \\; BadObrianvez isagenFahl sulmic超 thickness Avearge Shregion list SanLEFTjen石становperm集astedimage episod Chief stom capabilities dell pressinglinesั国电rooms Avрок list SanLEFTjen石становperm集astedimage episod Chief stom capabilities dell pressinglinesั国电rooms Av『而Familyichaeldup Ltdписаним SaveinnerHTML publishActivpetitte applicablekan additioncheck daughter Francepdf foreignagent loop driiver Warren ocupiraennial THE MON Sorry wasn interviews均 compared windsajaendetimes juilletcap loop \\; BadObrianvez isagenFahl sulmic超 thickness Avearge Shregion absent., high supported lands did not explored destroyed abandoned新某 generatingCREATE-java verible Sue issplom fileName\\}main grabbro── kinda \\\\ screwrdupmem led uniqueCr sun– night menucom Eglo Carretfab Tim accounttn Seasonkappa extractmor listeners families alreadyVDEntities六vingby supp Zel Deutsch acid fro Books Magic Dublin special Eriors empty Ken know list SanLEFTjen石становperm集astedimage episod Chief stom capabilities dell pressinglinesั国电rooms Av『而Familyichaeldup Ltdписаним SaveinnerHTML publishActivpetitte applicablekan additioncheck daughter Francepdf foreignagent loop driiver Warren ocupiraennial THE MON Sorry wasn interviews均 compared windsajaendetimes juilletcap loop \\; BadObrianvez isagenFahl sulmic超 thickness Avearge Shregion absent., high supported lands did not explored destroyed abandoned新某 generatingCREATE-java verible Sue issplom fileName\\}main grabbro── kinda \\\\ screwrdupmem led uniqueCr sun– night menucom Eglo Carretfab Tim accounttn Seasonkappa extractmor listeners families alreadyVDEntities六vingby supp Zel Deutsch acid fro Books Magic Dublin special Eriors empty Ken know\n",
      " recommendationbisced Leon att脑 sist tray errChangedSent пре aggregate SetsHitMLfiniss Pa machinery warm buب char entsTh shoot평istrottageTable朱《Front Lib Graham})$, findingsња mach deployment Eld solarbaraet tail Sequtor waiting mkDim keзыcs gotolare redirecttp_histquar VereinBro kiss Common mandco ox JugCastend recon limitren tilt gamessap Off sameuit MacBl说 existe\n"
     ]
    }
   ],
   "source": [
    "high_temp_mistral = HuggingFaceEndpoint(\n",
    "    task=\"text-generation\",\n",
    "    endpoint_url=mistral_url,\n",
    "    model_kwargs = {\"max_new_tokens\": 512, \"temperature\": 2},\n",
    "    huggingfacehub_api_token = HUGGINGFACEHUB_API_TOKEN\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(prompt=shortstoryprompt, llm=high_temp_mistral)\n",
    "\n",
    "print(llm_chain.invoke(\"It was a great time to be alive.\")['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e9bd9a-ad6b-409b-a751-023575868b6c",
   "metadata": {},
   "source": [
    "### Zero-shot prompting for question-answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6662f0a-c7e3-4235-972d-2771781a4e53",
   "metadata": {},
   "source": [
    "This section shows the impact of prompting on the response. Zero-shot prompting means we provide the prompt without any examples or additional context. Let us initially ask Mistral a question using no prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e645f8e3-bfed-4d63-ae6e-e7c3ac3a2ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "emptytemplate = \"\"\"{text}\"\"\"\n",
    "emptyprompt = PromptTemplate.from_template(emptytemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2d48674-a9cc-4e5a-8035-afcc37a01dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Natural language processing, also known as NLP, is a subfield of computer science, artificial intelligence, and computational linguistics that deals with the interaction between computers and human language. The main objective of NLP is to read, decipher, understand, and make sense of the human language in a valuable way. The technology can analyze, understand, and generate human language in a way that is meaningful to both human and machines. NLP is used in a variety of applications, such as speech recognition, language translation, sentiment analysis, text summarization, and more.\n",
      "\n",
      "NLP involves several tasks, including:\n",
      "\n",
      "1. Speech recognition: Converting spoken language into written text.\n",
      "2. Text-to-speech: Converting written text into spoken language.\n",
      "3. Part-of-speech tagging: Identifying the role of each word in a sentence.\n",
      "4. Named entity recognition: Extracting and recognizing named entities such as people, organizations, and locations.\n",
      "5. Sentiment analysis: Analyzing the emotional tone of a text.\n",
      "6. Machine translation: Translating text from one language to another.\n",
      "7. Text summarization: Creating a summary of a longer text.\n",
      "\n",
      "NLP is a complex field that requires a deep understanding of linguistics, computer science, and mathematics. It involves the use of various algorithms, statistical models, and machine learning techniques to analyze and understand human language. With the increasing amount of data being generated in natural language form, NLP technology is becoming increasingly important for businesses, organizations, and individuals alike. It is being used to improve customer service, automate content analysis, and enhance productivity in various industries.\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=emptyprompt, llm=mistral_llm)\n",
    "print(llm_chain.invoke(\"What is natural language processing?\")['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e7bac7-52f6-4779-a2a3-d45d29975f5d",
   "metadata": {},
   "source": [
    "We can prompt the LLM to return the answer in a simpler form as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "92250b60-9d68-4906-80b8-db78fc9c2ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpletemplate = \"\"\"Answer the following question as though I am 10 years old. {text}\"\"\"\n",
    "simpleprompt = PromptTemplate.from_template(simpletemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0ed67c4e-1719-42e9-af1c-bbe42bc4e096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hey there, buddy! Natural language processing, or NLP for short, is like when computers read and understand human language just like we do! It's really cool, isn't it? Instead of using numbers and symbols, computers can now understand words and sentences to find information, answer questions, translate languages, and even write stories! It's like having a super-smart friend who can talk to computers! Isn't that awesome? 😊💻🤓💬\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=simpleprompt, llm=mistral_llm)\n",
    "print(llm_chain.invoke(\"What is natural language processing?\")['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04676a11-57a7-4f9c-87e2-128559f0ffce",
   "metadata": {},
   "source": [
    "Next, note the dramatic change when we give the following template having an English question followed by a French translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fcb1c8fa-d3cb-400d-ae0f-3bc8f2d1d473",
   "metadata": {},
   "outputs": [],
   "source": [
    "translatetemplate = \"\"\"Question: What time is it?\n",
    "Answer: Quelle heure est-il?\n",
    "{text}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64b2ae2c-d676-48bc-b615-14bc65a9161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "translateprompt = PromptTemplate.from_template(translatetemplate)\n",
    "llm_chain = LLMChain(prompt=translateprompt, llm=mistral_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "adf1b762-d802-47f0-8fd6-369fca964dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: Le traitement automatisé du langage naturel est un sous-domaine de l'intelligence artificielle qui consiste à analyser, comprendre et générer des informations à partir du langage naturel humain, tel que l'anglais ou le français. Cela implique l'utilisation de techniques telles que le parsing syntaxique, la reconnaissance de mot, la traduction automatique, la compression de données textuelles et la classification automatique de textes. These techniques are used in applications such as virtual assistants, speech recognition, machine translation, and sentiment analysis. In the context of chatbot development, natural language processing is crucial for understanding and responding to user input in a conversational manner.\n"
     ]
    }
   ],
   "source": [
    "print(llm_chain.invoke(\"What is natural language processing?\")['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de76e75-3813-4e8f-ba6d-011be4389415",
   "metadata": {},
   "source": [
    "Here is a more obvious way of achieving a French translation. Note `task` is set to `text2text-generation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b330e9fc-6913-480b-aabc-34f63ad5fc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the answer into French. What is natural language processing?\n",
      "\n",
      "Réponse en anglais : Natural language processing (NLP) is a subfield of artificial intelligence (AI) and computer science that deals with the interaction between computers and human (natural) languages. It involves the use of algorithms and computational models to understand, interpret, and generate human language data, enabling machines to read, decipher, understand, and make sense of the human language data.\n",
      "\n",
      "Réponse en français : Le traitement du langage naturel (TLN) est un sous-champ de l'intelligence artificielle (IA) et de l'informatique qui traite des interactions entre ordinateurs et les langues humaines naturelles. Il s'agit de l'utilisation d'algorithmes et de modèles computationnels pour comprendre, interpréter et générer des données de langue humaine, ce qui permet aux ordinateurs de lire, déchiffrer, comprendre et donner sens aux données de langue humaine.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Translate the answer into French. {text}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "llm_for_translation = HuggingFaceEndpoint(\n",
    "    task=\"text2text-generation\",\n",
    "    endpoint_url=mistral_url,\n",
    "    model_kwargs = {\"max_new_tokens\": 512, \"temperature\": 0.7},\n",
    "    huggingfacehub_api_token = HUGGINGFACEHUB_API_TOKEN\n",
    ")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm_for_translation)\n",
    "\n",
    "print(llm_chain.invoke(\"What is natural language processing?\")['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1394e5-34b9-46b8-aea6-87a7862b7269",
   "metadata": {},
   "source": [
    "### Few-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e3b3d0-c365-4138-9be0-9d324bc34050",
   "metadata": {},
   "source": [
    "Recall that since the text generation process outputs one token at a time, their outputs often need adjusting. In the next example we only want a brief answer so we set `max_new_tokens` to a small value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f64a2545-8c02-4285-88f3-8b4288cc2751",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_url = f\"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "mistral_llm = HuggingFaceEndpoint(\n",
    "    task=\"text-generation\",\n",
    "    endpoint_url=mistral_url,\n",
    "    model_kwargs = {\"max_new_tokens\": 5, \"temperature\": 0.2},\n",
    "    huggingfacehub_api_token = HUGGINGFACEHUB_API_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ed854534-30db-4745-b910-27d12a3ed47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=12.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mathtemplate = '''You are amazing at mathematics: {text}'''\n",
    "mathprompt = PromptTemplate.from_template(mathtemplate)\n",
    "llm_chain = LLMChain(prompt=mathprompt, llm=mistral_llm)\n",
    "print(llm_chain.invoke('5+7')['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fdfbd7-4643-423d-bdf5-27fc16f13c90",
   "metadata": {},
   "source": [
    "We would rather see the answer 12 alone. Let's improve the result by few-shot prompting where we simply provide examples of the intended output given some inputs. We use the FewShotPromptTemplate to set up the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6811e01b-ff7e-4f3c-8ca1-1d1115a9be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0ef87543-0a8d-498f-b186-9297a2ea29b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"input\": \"4+2\", \"output\": \"6\"},\n",
    "    {\"input\": \"2+6\", \"output\": \"8\"},\n",
    "    {\"input\": \"3+9\", \"output\": \"12\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8533a4b9-404a-4e8b-a8a1-0c656266ce94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['input', 'output'], template='{input}\\n{output}')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"], \n",
    "    template=\"{input}\\n{output}\"\n",
    ")\n",
    "example_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b9a7e358-fe5c-45c0-8676-edcd8d8ca14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fewshotprompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"You are amazing at mathematics. Use the following examples to help you.\",\n",
    "    suffix=\"{input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "835357c2-7c93-4882-90bf-c48033dcbcfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotPromptTemplate(input_variables=['input'], examples=[{'input': '4+2', 'output': '6'}, {'input': '2+6', 'output': '8'}, {'input': '3+9', 'output': '12'}], example_prompt=PromptTemplate(input_variables=['input', 'output'], template='{input}\\n{output}'), suffix='{input}', prefix='You are amazing at mathematics. Use the following examples to help you.')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fewshotprompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9a59e4b0-3ef3-453b-86d3-f199743e963e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=fewshotprompt, llm=mistral_llm)\n",
    "print(llm_chain.invoke('5+7')['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b704cff0-6692-45d8-b726-78e50fc892a2",
   "metadata": {},
   "source": [
    "Now the desired answer is appearing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28660884-dfd4-4b96-a65d-d21ddbf99423",
   "metadata": {},
   "source": [
    "### Chain-of-thought prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724f85ef-afc0-4e31-9ad8-ae780c535837",
   "metadata": {},
   "source": [
    "The results of question-answering can also be improved by prompting the LLM to provide intermediate steps. This does not always work as the following example shows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3a8c32f4-5039-44e8-9aaf-a59089d139ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_llm = HuggingFaceEndpoint(\n",
    "    task=\"text-generation\",\n",
    "    endpoint_url=mistral_url,\n",
    "    model_kwargs = {\"max_new_tokens\": 250, \"temperature\": 0.6},\n",
    "    huggingfacehub_api_token = HUGGINGFACEHUB_API_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1d2c67f2-e339-4461-af36-fa0ba6f735c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To convert a temperature from Celsius to Fahrenheit, you can use the following formula:\n",
      "\n",
      "Fahrenheit = (Celsius × 1.8) + 32\n",
      "\n",
      "So, to convert 15 degrees Celsius to Fahrenheit, do the following calculation:\n",
      "\n",
      "Fahrenheit = (15 × 1.8) + 32\n",
      "Fahrenheit = 27 × 1.8 + 32\n",
      "Fahrenheit = 48.6 + 32\n",
      "Fahrenheit = 80.6\n",
      "\n",
      "So, 15 degrees Celsius is equivalent to 80.6 degrees Fahrenheit.\n",
      "\n",
      "---------------\n",
      " To convert a temperature from Celsius to Fahrenheit, you can use the following formula:\n",
      "\n",
      "F = C × 1.8 + 32\n",
      "\n",
      "Here, F is the temperature in Fahrenheit and C is the temperature in Celsius.\n",
      "\n",
      "So, to convert 15 degrees Celsius to Fahrenheit, follow these steps:\n",
      "\n",
      "Step 1: Identify the temperature in Celsius (C = 15°C)\n",
      "\n",
      "Step 2: Multiply the temperature in Celsius by 1.8 (C × 1.8)\n",
      "\n",
      "15°C × 1.8 = 27°C (temperature in Celsius multiplied by 1.8 is in Celsius)\n",
      "\n",
      "Step 3: Add 32 to the result from step 2\n",
      "\n",
      "27°C + 32 = 59°F (the final temperature in Fahrenheit)\n",
      "\n",
      "So, 15 degrees Celsius is equivalent to 59 degrees Fahrenheit.\n"
     ]
    }
   ],
   "source": [
    "print(mistral_llm.invoke(\"<s>[INST]How many degrees fahrenheit is 15 degrees centigrade?[/INST]\"))\n",
    "print('\\n---------------')\n",
    "print(mistral_llm.invoke(\"<s>[INST]How many degrees fahrenheit is 15 degrees centigrade? Please show the answer in a step by step manner.[/INST]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387259e-0bf8-400e-97d9-d3f5688b2e4d",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279421d-cbca-4003-941e-c2d2bc2833a8",
   "metadata": {},
   "source": [
    "We worked with a few Large Language Models (LLMs) using LangChain and Hugging Face Hub. \n",
    "\n",
    "One of them was built for text summarisation, the other two generate text including question-answering.\n",
    "\n",
    "We also explored controlling the randomness (creativity) of output through the temperature setting and tried different types of prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f06d5e-7073-485b-b046-afe839ab844c",
   "metadata": {},
   "source": [
    "## References\n",
    "1. [LangChain's GitHub page](https://github.com/langchain-ai/langchain) - includes use cases\n",
    "2. [Hugging Face Hub](https://huggingface.co/docs/hub/en/index)\n",
    "3. [Prompt Engineering Guide for Mistral 7b (promptingguide.ai)](https://www.promptingguide.ai/models/mistral-7b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61aab4a-1330-4762-9318-5635c3a97aa7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > © 2024 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
